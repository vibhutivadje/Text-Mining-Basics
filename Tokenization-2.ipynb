{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/ashutoshfolane/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download_shell()\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenizing sentence and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the sentence and word tokenization module\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some sample texts\n",
    "sample_text='''\n",
    "Trump was born and raised in the New York City borough of Queens,\n",
    "and received an economics degree from the Wharton School of the University of\n",
    "Pennsylvania. He took charge of his family's real estate business in 1971,\n",
    "renamed it to The Trump Organization, and expanded it into Manhattan.\n",
    "The company built or renovated skyscrapers, hotels, casinos, and golf courses.\n",
    "Trump later started various side ventures, including licensing his name\n",
    "for real estate and consumer products. He managed the company until his\n",
    "2017 inauguration. He co-authored several books, including The Art of the Deal.\n",
    "He owned the Miss Universe and Miss USA beauty pageants from 1996 to 2015,\n",
    "and he produced and hosted the reality television show The Apprentice from\n",
    "2003 to 2015. Forbes estimates his net worth to be $3.1 billion. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trump was born and raised in the New York City borough of Queens,\n",
      "and received an economics degree from the Wharton School of the University of\n",
      "Pennsylvania.\n",
      "\n",
      "He took charge of his family's real estate business in 1971,\n",
      "renamed it to The Trump Organization, and expanded it into Manhattan.\n",
      "\n",
      "The company built or renovated skyscrapers, hotels, casinos, and golf courses.\n",
      "\n",
      "Trump later started various side ventures, including licensing his name\n",
      "for real estate and consumer products.\n",
      "\n",
      "He managed the company until his\n",
      "2017 inauguration.\n",
      "\n",
      "He co-authored several books, including The Art of the Deal.\n",
      "\n",
      "He owned the Miss Universe and Miss USA beauty pageants from 1996 to 2015,\n",
      "and he produced and hosted the reality television show The Apprentice from\n",
      "2003 to 2015.\n",
      "\n",
      "Forbes estimates his net worth to be $3.1 billion.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the corpora into a list of sentences\n",
    "# sent = sent_tokenize(sample_text)#creates a list of sentence\n",
    "# sent\n",
    "for i in sent_tokenize(sample_text):\n",
    "    print(i+'\\n')\n",
    "#       print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trump', 'was', 'born', 'and', 'raised', 'in', 'the', 'New', 'York', 'City', 'borough', 'of', 'Queens', ',', 'and', 'received', 'an', 'economics', 'degree', 'from', 'the', 'Wharton', 'School', 'of', 'the', 'University', 'of', 'Pennsylvania', '.', 'He', 'took', 'charge', 'of', 'his', 'family', \"'s\", 'real', 'estate', 'business', 'in', '1971', ',', 'renamed', 'it', 'to', 'The', 'Trump', 'Organization', ',', 'and', 'expanded', 'it', 'into', 'Manhattan', '.', 'The', 'company', 'built', 'or', 'renovated', 'skyscrapers', ',', 'hotels', ',', 'casinos', ',', 'and', 'golf', 'courses', '.', 'Trump', 'later', 'started', 'various', 'side', 'ventures', ',', 'including', 'licensing', 'his', 'name', 'for', 'real', 'estate', 'and', 'consumer', 'products', '.', 'He', 'managed', 'the', 'company', 'until', 'his', '2017', 'inauguration', '.', 'He', 'co-authored', 'several', 'books', ',', 'including', 'The', 'Art', 'of', 'the', 'Deal', '.', 'He', 'owned', 'the', 'Miss', 'Universe', 'and', 'Miss', 'USA', 'beauty', 'pageants', 'from', '1996', 'to', '2015', ',', 'and', 'he', 'produced', 'and', 'hosted', 'the', 'reality', 'television', 'show', 'The', 'Apprentice', 'from', '2003', 'to', '2015', '.', 'Forbes', 'estimates', 'his', 'net', 'worth', 'to', 'be', '$', '3.1', 'billion', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the corpora into a list of words\n",
    "print(word_tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trump', 'was', 'born', 'and', 'raised', 'in', 'the', 'New', 'York', 'City', 'borough', 'of', 'Queens', ',', 'and', 'received', 'an', 'economics', 'degree', 'from', 'the', 'Wharton', 'School', 'of', 'the', 'University', 'of', 'Pennsylvania', '.'] \n",
      "\n",
      "['He', 'took', 'charge', 'of', 'his', 'family', \"'s\", 'real', 'estate', 'business', 'in', '1971', ',', 'renamed', 'it', 'to', 'The', 'Trump', 'Organization', ',', 'and', 'expanded', 'it', 'into', 'Manhattan', '.'] \n",
      "\n",
      "['The', 'company', 'built', 'or', 'renovated', 'skyscrapers', ',', 'hotels', ',', 'casinos', ',', 'and', 'golf', 'courses', '.'] \n",
      "\n",
      "['Trump', 'later', 'started', 'various', 'side', 'ventures', ',', 'including', 'licensing', 'his', 'name', 'for', 'real', 'estate', 'and', 'consumer', 'products', '.'] \n",
      "\n",
      "['He', 'managed', 'the', 'company', 'until', 'his', '2017', 'inauguration', '.'] \n",
      "\n",
      "['He', 'co-authored', 'several', 'books', ',', 'including', 'The', 'Art', 'of', 'the', 'Deal', '.'] \n",
      "\n",
      "['He', 'owned', 'the', 'Miss', 'Universe', 'and', 'Miss', 'USA', 'beauty', 'pageants', 'from', '1996', 'to', '2015', ',', 'and', 'he', 'produced', 'and', 'hosted', 'the', 'reality', 'television', 'show', 'The', 'Apprentice', 'from', '2003', 'to', '2015', '.'] \n",
      "\n",
      "['Forbes', 'estimates', 'his', 'net', 'worth', 'to', 'be', '$', '3.1', 'billion', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the list of words for each sentence\n",
    "for i in sent_tokenize(sample_text):\n",
    "    print(word_tokenize(i),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trump was born and raised in the New York City borough of Queens,\n",
      "and received an economics degree from the Wharton School of the University of\n",
      "Pennsylvania.\n",
      "158\n",
      "['Trump', 'was', 'born', 'and', 'raised', 'in', 'the', 'New', 'York', 'City', 'borough', 'of', 'Queens', ',', 'and', 'received', 'an', 'economics', 'degree', 'from', 'the', 'Wharton', 'School', 'of', 'the', 'University', 'of', 'Pennsylvania', '.']\n",
      "29\n",
      "He took charge of his family's real estate business in 1971,\n",
      "renamed it to The Trump Organization, and expanded it into Manhattan.\n",
      "130\n",
      "['He', 'took', 'charge', 'of', 'his', 'family', \"'s\", 'real', 'estate', 'business', 'in', '1971', ',', 'renamed', 'it', 'to', 'The', 'Trump', 'Organization', ',', 'and', 'expanded', 'it', 'into', 'Manhattan', '.']\n",
      "26\n",
      "The company built or renovated skyscrapers, hotels, casinos, and golf courses.\n",
      "78\n",
      "['The', 'company', 'built', 'or', 'renovated', 'skyscrapers', ',', 'hotels', ',', 'casinos', ',', 'and', 'golf', 'courses', '.']\n",
      "15\n",
      "Trump later started various side ventures, including licensing his name\n",
      "for real estate and consumer products.\n",
      "110\n",
      "['Trump', 'later', 'started', 'various', 'side', 'ventures', ',', 'including', 'licensing', 'his', 'name', 'for', 'real', 'estate', 'and', 'consumer', 'products', '.']\n",
      "18\n",
      "He managed the company until his\n",
      "2017 inauguration.\n",
      "51\n",
      "['He', 'managed', 'the', 'company', 'until', 'his', '2017', 'inauguration', '.']\n",
      "9\n",
      "He co-authored several books, including The Art of the Deal.\n",
      "60\n",
      "['He', 'co-authored', 'several', 'books', ',', 'including', 'The', 'Art', 'of', 'the', 'Deal', '.']\n",
      "12\n",
      "He owned the Miss Universe and Miss USA beauty pageants from 1996 to 2015,\n",
      "and he produced and hosted the reality television show The Apprentice from\n",
      "2003 to 2015.\n",
      "163\n",
      "['He', 'owned', 'the', 'Miss', 'Universe', 'and', 'Miss', 'USA', 'beauty', 'pageants', 'from', '1996', 'to', '2015', ',', 'and', 'he', 'produced', 'and', 'hosted', 'the', 'reality', 'television', 'show', 'The', 'Apprentice', 'from', '2003', 'to', '2015', '.']\n",
      "31\n",
      "Forbes estimates his net worth to be $3.1 billion.\n",
      "50\n",
      "['Forbes', 'estimates', 'his', 'net', 'worth', 'to', 'be', '$', '3.1', 'billion', '.']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# check the number of characters and the number of words.\n",
    "for i in nltk.sent_tokenize(sample_text):\n",
    "    print(i)\n",
    "    print(len(i)) # number of characters\n",
    "    x=nltk.word_tokenize(i)\n",
    "    print(x)\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 24 \n",
      "\n",
      "26 24 \n",
      "\n",
      "15 13 \n",
      "\n",
      "18 18 \n",
      "\n",
      "9 9 \n",
      "\n",
      "12 12 \n",
      "\n",
      "31 24 \n",
      "\n",
      "11 11 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the set of each tokenized sentence\n",
    "for i in sent_tokenize(sample_text):\n",
    "    words=word_tokenize(i)\n",
    "    instancecount=len(words)\n",
    "    typecount=len(set(words))#set is used to calculate the number of unique words in the sentence.\n",
    "    print(instancecount,typecount,'\\n') # the numbers are different since there are repeated words\n",
    "# how many duplicated words in the last sentence?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'c', 'a']\n",
      "{'c': None, 'b': None, 'a': None}\n",
      "['c', 'b', 'a']\n"
     ]
    }
   ],
   "source": [
    "#Generate a list of unique words\n",
    "x=['c','b','a','c','c','b']\n",
    "print(list(set(x))) # this will randomize the sequence. therefore it will rearrange the words in the sentence.\n",
    "y=dict.fromkeys(x) # generate a empty dictionary from the values in x. This will retain the sequence\n",
    "print(y)\n",
    "print(list(dict.fromkeys(x))) # this will remove the duplicate with the original sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trump', 'was', 'born', 'and', 'raised', 'in', 'the', 'New', 'York', 'City', 'borough', 'of', 'Queens', ',', 'received', 'an', 'economics', 'degree', 'from', 'Wharton', 'School', 'University', 'Pennsylvania', '.', 'He', 'took', 'charge', 'his', 'family', \"'s\", 'real', 'estate', 'business', '1971', 'renamed', 'it', 'to', 'The', 'Organization', 'expanded', 'into', 'Manhattan', 'company', 'built', 'or', 'renovated', 'skyscrapers', 'hotels', 'casinos', 'golf', 'courses', 'later', 'started', 'various', 'side', 'ventures', 'including', 'licensing', 'name', 'for', 'consumer', 'products', 'managed', 'until', '2017', 'inauguration', 'co-authored', 'several', 'books', 'Art', 'Deal', 'owned', 'Miss', 'Universe', 'USA', 'beauty', 'pageants', '1996', '2015', 'he', 'produced', 'hosted', 'reality', 'television', 'show', 'Apprentice', '2003', 'Forbes', 'estimates', 'net', 'worth', 'be', '$', '3.1', 'billion']\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "wordlist=word_tokenize(sample_text)\n",
    "#wordlist\n",
    "#print(len(list(set(wordlist))))\n",
    "unique_word_list=list(dict.fromkeys(wordlist))\n",
    "#unique_word_list\n",
    "print(unique_word_list)\n",
    "print(len(unique_word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. nltk.freqdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Trump': 3, 'was': 1, 'born': 1, 'and': 8, 'raised': 1, 'in': 2, 'the': 7, 'New': 1, 'York': 1, 'City': 1, 'borough': 1, 'of': 5, 'Queens': 1, ',': 9, 'received': 1, 'an': 1, 'economics': 1, 'degree': 1, 'from': 3, 'Wharton': 1, 'School': 1, 'University': 1, 'Pennsylvania': 1, '.': 8, 'He': 4, 'took': 1, 'charge': 1, 'his': 4, 'family': 1, \"'s\": 1, 'real': 2, 'estate': 2, 'business': 1, '1971': 1, 'renamed': 1, 'it': 2, 'to': 4, 'The': 4, 'Organization': 1, 'expanded': 1, 'into': 1, 'Manhattan': 1, 'company': 2, 'built': 1, 'or': 1, 'renovated': 1, 'skyscrapers': 1, 'hotels': 1, 'casinos': 1, 'golf': 1, 'courses': 1, 'later': 1, 'started': 1, 'various': 1, 'side': 1, 'ventures': 1, 'including': 2, 'licensing': 1, 'name': 1, 'for': 1, 'consumer': 1, 'products': 1, 'managed': 1, 'until': 1, '2017': 1, 'inauguration': 1, 'co-authored': 1, 'several': 1, 'books': 1, 'Art': 1, 'Deal': 1, 'owned': 1, 'Miss': 2, 'Universe': 1, 'USA': 1, 'beauty': 1, 'pageants': 1, '1996': 1, '2015': 2, 'he': 1, 'produced': 1, 'hosted': 1, 'reality': 1, 'television': 1, 'show': 1, 'Apprentice': 1, '2003': 1, 'Forbes': 1, 'estimates': 1, 'net': 1, 'worth': 1, 'be': 1, '$': 1, '3.1': 1, 'billion': 1}\n"
     ]
    }
   ],
   "source": [
    "# this is how to count the frequency of each words\n",
    "# let's do this without the freqdist function\n",
    "\n",
    "x=nltk.word_tokenize(sample_text)\n",
    "y=dict.fromkeys(x)\n",
    "for key in y:\n",
    "    y[key]=x.count(key)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 3 samples and 8 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog -> 4\n",
      "cat -> 3\n",
      "snake -> 1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
    "#fdist prints a dict with unique words and their count\n",
    "for word in fdist:\n",
    "    print(word,'->',fdist[word]) #the frequency of a word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dog', 4), ('cat', 3), ('snake', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(fdist.most_common())#prints fro most freq - least freq. DS: list of tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "dog\n",
      "[('dog', 4), ('cat', 3), ('snake', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(fdist['dog']) # get the frequency of a word\n",
    "print(fdist.max()) # get the most frequent word\n",
    "print(fdist.most_common(5)) # the two most common words, #prints fro most freq - least freq. DS: list of tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text='''\n",
    "Trump was born and raised in the New York City borough of Queens,\n",
    "and received an economics degree from the Wharton School of the University of\n",
    "Pennsylvania. He took charge of his family's real estate business in 1971,\n",
    "renamed it to The Trump Organization, and expanded it into Manhattan.\n",
    "The company built or renovated skyscrapers, hotels, casinos, and golf courses.\n",
    "Trump later started various side ventures, including licensing his name\n",
    "for real estate and consumer products. He managed the company until his\n",
    "2017 inauguration. He co-authored several books, including The Art of the Deal.\n",
    "He owned the Miss Universe and Miss USA beauty pageants from 1996 to 2015,\n",
    "and he produced and hosted the reality television show The Apprentice from\n",
    "2003 to 2015. Forbes estimates his net worth to be $3.1 billion. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", -> 9\n",
      "and -> 8\n",
      ". -> 8\n",
      "the -> 7\n",
      "of -> 5\n",
      "He -> 4\n",
      "his -> 4\n",
      "to -> 4\n",
      "The -> 4\n",
      "Trump -> 3\n",
      "from -> 3\n",
      "in -> 2\n",
      "real -> 2\n",
      "estate -> 2\n",
      "it -> 2\n",
      "company -> 2\n",
      "including -> 2\n",
      "Miss -> 2\n",
      "2015 -> 2\n",
      "was -> 1\n",
      "born -> 1\n",
      "raised -> 1\n",
      "New -> 1\n",
      "York -> 1\n",
      "City -> 1\n",
      "borough -> 1\n",
      "Queens -> 1\n",
      "received -> 1\n",
      "an -> 1\n",
      "economics -> 1\n",
      "degree -> 1\n",
      "Wharton -> 1\n",
      "School -> 1\n",
      "University -> 1\n",
      "Pennsylvania -> 1\n",
      "took -> 1\n",
      "charge -> 1\n",
      "family -> 1\n",
      "'s -> 1\n",
      "business -> 1\n",
      "1971 -> 1\n",
      "renamed -> 1\n",
      "Organization -> 1\n",
      "expanded -> 1\n",
      "into -> 1\n",
      "Manhattan -> 1\n",
      "built -> 1\n",
      "or -> 1\n",
      "renovated -> 1\n",
      "skyscrapers -> 1\n",
      "hotels -> 1\n",
      "casinos -> 1\n",
      "golf -> 1\n",
      "courses -> 1\n",
      "later -> 1\n",
      "started -> 1\n",
      "various -> 1\n",
      "side -> 1\n",
      "ventures -> 1\n",
      "licensing -> 1\n",
      "name -> 1\n",
      "for -> 1\n",
      "consumer -> 1\n",
      "products -> 1\n",
      "managed -> 1\n",
      "until -> 1\n",
      "2017 -> 1\n",
      "inauguration -> 1\n",
      "co-authored -> 1\n",
      "several -> 1\n",
      "books -> 1\n",
      "Art -> 1\n",
      "Deal -> 1\n",
      "owned -> 1\n",
      "Universe -> 1\n",
      "USA -> 1\n",
      "beauty -> 1\n",
      "pageants -> 1\n",
      "1996 -> 1\n",
      "he -> 1\n",
      "produced -> 1\n",
      "hosted -> 1\n",
      "reality -> 1\n",
      "television -> 1\n",
      "show -> 1\n",
      "Apprentice -> 1\n",
      "2003 -> 1\n",
      "Forbes -> 1\n",
      "estimates -> 1\n",
      "net -> 1\n",
      "worth -> 1\n",
      "be -> 1\n",
      "$ -> 1\n",
      "3.1 -> 1\n",
      "billion -> 1\n",
      "[(',', 9), ('and', 8), ('.', 8), ('the', 7), ('of', 5)]\n"
     ]
    }
   ],
   "source": [
    "word_list=nltk.word_tokenize(sample_text)\n",
    "fdist=nltk.FreqDist(word_list)\n",
    "for i in fdist:\n",
    "    print(i,'->',fdist[i])\n",
    "print(fdist.most_common(5)) # you may see that the most frequent words are almost certain meaningless.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The stop words in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#used to connect sentences but not used to convey meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words=stopwords.words(\"english\") # you may also try french, german\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the stop words from the corpora\n",
    "\n",
    "sample_text='''\n",
    "Trump was born and raised in the New York City borough of Queens,\n",
    "and received an economics degree from the Wharton School of the University of\n",
    "Pennsylvania. He took charge of his family's real estate business in 1971,\n",
    "renamed it to The Trump Organization, and expanded it into Manhattan.\n",
    "The company built or renovated skyscrapers, hotels, casinos, and golf courses.\n",
    "Trump later started various side ventures, including licensing his name\n",
    "for real estate and consumer products. He managed the company until his\n",
    "2017 inauguration. He co-authored several books, including The Art of the Deal.\n",
    "He owned the Miss Universe and Miss USA beauty pageants from 1996 to 2015,\n",
    "and he produced and hosted the reality television show The Apprentice from\n",
    "2003 to 2015. Forbes estimates his net worth to be $3.1 billion. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trump', 'born', 'raised', 'New', 'York', 'City', 'borough', 'Queens', ',', 'received', 'economics', 'degree', 'Wharton', 'School', 'University', 'Pennsylvania', '.', 'He', 'took', 'charge', 'family', \"'s\", 'real', 'estate', 'business', '1971', ',', 'renamed', 'The', 'Trump', 'Organization', ',', 'expanded', 'Manhattan', '.', 'The', 'company', 'built', 'renovated', 'skyscrapers', ',', 'hotels', ',', 'casinos', ',', 'golf', 'courses', '.', 'Trump', 'later', 'started', 'various', 'side', 'ventures', ',', 'including', 'licensing', 'name', 'real', 'estate', 'consumer', 'products', '.', 'He', 'managed', 'company', '2017', 'inauguration', '.', 'He', 'co-authored', 'several', 'books', ',', 'including', 'The', 'Art', 'Deal', '.', 'He', 'owned', 'Miss', 'Universe', 'Miss', 'USA', 'beauty', 'pageants', '1996', '2015', ',', 'produced', 'hosted', 'reality', 'television', 'show', 'The', 'Apprentice', '2003', '2015', '.', 'Forbes', 'estimates', 'net', 'worth', '$', '3.1', 'billion', '.']\n"
     ]
    }
   ],
   "source": [
    "words=word_tokenize(sample_text)\n",
    "filtered_sample=[w for w in words if not w in stop_words] # [w for w in words if w not in stop_words] also works\n",
    "# filtered=[w for w in words if w.lower() not in stop_words and w not in string.punctuation]\n",
    "#If you want only alphabets, can use isalpha() to remove punctuation and stopwords\n",
    "# filtered_n = [w for w in words if w.lower() not in stop_words and w.isalpha()]\n",
    "print(filtered_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 9), ('.', 8), ('He', 4), ('The', 4), ('Trump', 3)]\n"
     ]
    }
   ],
   "source": [
    "# now check the most frequent words after removing stop words\n",
    "import nltk\n",
    "fdist=nltk.FreqDist(filtered_sample)\n",
    "print(fdist.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Trump was born raised in New York City borough Queens received an economics degree from Wharton School University Pennsylvania He took charge his family 's real estate business in 1971 renamed it to The Trump Organization expanded it into Manhattan The company built or renovated skyscrapers hotels casinos golf courses Trump later started various side ventures including licensing his name for real estate consumer products He managed company until his 2017 inauguration He co-authored several books including The Art Deal He owned Miss Universe Miss USA beauty pageants from 1996 to 2015 he produced hosted reality television show The Apprentice from 2003 to 2015 Forbes estimates his net worth to be $ 3.1 billion\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist=nltk.word_tokenize(sample_text)\n",
    "fdist=nltk.FreqDist(wordlist)\n",
    "to_remove = [x for x,y in fdist.most_common(5)]\n",
    "# to_remove\n",
    "filtered = [x for x in wordlist if x not in to_remove]\n",
    "filtered_text = \" \".join(filtered)\n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# now remove the punctuations\n",
    "import string\n",
    "print(string.punctuation) # these are the punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 4), ('The', 4), ('Trump', 3), ('real', 2), ('estate', 2)]\n"
     ]
    }
   ],
   "source": [
    "filtered_sample_2=[w for w in words if not w in stop_words and not w in string.punctuation]\n",
    "fdist=nltk.FreqDist(filtered_sample_2)\n",
    "print(fdist.most_common(5)) # now we see that real estate seems to be very frequent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still have the 'He', 'The'. Need to lower case. Make the following change\n",
    "filtered=[w for w in words if w.lower() not in stop_words and w not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trump', 'born', 'raised', 'New', 'York', 'City', 'borough', 'Queens', 'received', 'economics', 'degree', 'Wharton', 'School', 'University', 'Pennsylvania', 'took', 'charge', 'family', 'real', 'estate', 'business', 'renamed', 'Trump', 'Organization', 'expanded', 'Manhattan', 'company', 'built', 'renovated', 'skyscrapers', 'hotels', 'casinos', 'golf', 'courses', 'Trump', 'later', 'started', 'various', 'side', 'ventures', 'including', 'licensing', 'name', 'real', 'estate', 'consumer', 'products', 'managed', 'company', 'inauguration', 'several', 'books', 'including', 'Art', 'Deal', 'owned', 'Miss', 'Universe', 'Miss', 'USA', 'beauty', 'pageants', 'produced', 'hosted', 'reality', 'television', 'show', 'Apprentice', 'Forbes', 'estimates', 'net', 'worth', 'billion']\n"
     ]
    }
   ],
   "source": [
    "#If you want only alphabets, can use isalpha() to remove punctuation and stopwords\n",
    "filtered_n = [w for w in words if w.lower() not in stop_words and w.isalpha()]\n",
    "print(filtered_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords -> remove punc and digits -> stemming -> remove background noise.......\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1=[\"game\",\"gaming\",\"gamed\",\"games\"]\n",
    "word2=[\"learn\",\"learning\",\"learns\",\"learned\",\"learner\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game\n",
      "game\n",
      "game\n",
      "game\n"
     ]
    }
   ],
   "source": [
    "for word in word1:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn\n",
      "learn\n",
      "learn\n",
      "learn\n",
      "learner\n"
     ]
    }
   ],
   "source": [
    "for word in word2:\n",
    "    print(stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It:It\n",
      "is:is\n",
      "important:import\n",
      "to:to\n",
      "learn:learn\n",
      "what:what\n",
      "you:you\n",
      "are:are\n",
      "learning:learn\n",
      ".:.\n"
     ]
    }
   ],
   "source": [
    "# Stemming a sentence\n",
    "example_sentence=\"It is important to learn what you are learning.\"\n",
    "words=word_tokenize(example_sentence)\n",
    "ps = PorterStemmer()\n",
    "for w in words:\n",
    "    print(w+':'+ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fli gener die mule die agre own meet item tradit refer\n"
     ]
    }
   ],
   "source": [
    "# let's check the accuracy of the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "plurals = ['flies','generously','dies','mules',\n",
    "'died', 'agreed', 'owned','meeting','itemization',\n",
    "'traditional','reference']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles)) # now we know that it is sometimes not accurate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out other stemmers\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "plurals = ['flies','generously','dies','mules',\n",
    "'died', 'agreed', 'owned','meeting','itemization',\n",
    "'traditional','reference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer1=PorterStemmer()\n",
    "stemmer2=SnowballStemmer(\"english\")\n",
    "stemmer3=nltk.LancasterStemmer()#we have different stemmers. \n",
    "stemmer4=nltk.WordNetLemmatizer()#WordNet lemmatizer removes affixes only if the resulting word is in its dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "singles1=[stemmer1.stem(plural) for plural in plurals]\n",
    "singles2=[stemmer2.stem(plural) for plural in plurals]\n",
    "singles3=[stemmer3.stem(plural) for plural in plurals]\n",
    "singles4=[stemmer4.lemmatize(plural) for plural in plurals]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fli gener die mule die agre own meet item tradit refer\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(singles1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fli generous die mule die agre own meet item tradit refer\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(singles2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fli gen die mul died agree own meet item tradit ref\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(singles3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly generously dy mule died agreed owned meeting itemization traditional reference\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(singles4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'item', 'do', 'list', 'each', 'item', 'very', 'well']\n"
     ]
    }
   ],
   "source": [
    "# manual stemmer\n",
    "# you can use this to correct some key mistakes from the other stemmer\n",
    "import nltk\n",
    "dictionary={'did':'do','itemization':'item'}\n",
    "sentence='the itemization did list each item very well'\n",
    "wordlist=nltk.word_tokenize(sentence)\n",
    "stemmed=[dictionary.get(i,i) for i in wordlist]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=\"Part of Speech tagging does exactly what it sounds like, it tags each word in a sentence with the part of speech for that word. This means it labels words as noun, adjective, verb, etc. PoS tagging also covers tenses of the parts of speech.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Part', 'NN'), ('of', 'IN'), ('Speech', 'NNP'), ('tagging', 'NN'), ('does', 'VBZ'), ('exactly', 'RB'), ('what', 'WP'), ('it', 'PRP'), ('sounds', 'VBZ'), ('like', 'IN'), (',', ','), ('it', 'PRP'), ('tags', 'VBZ'), ('each', 'DT'), ('word', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('with', 'IN'), ('the', 'DT'), ('part', 'NN'), ('of', 'IN'), ('speech', 'NN'), ('for', 'IN'), ('that', 'DT'), ('word', 'NN'), ('.', '.')]\n",
      "[('This', 'DT'), ('means', 'VBZ'), ('it', 'PRP'), ('labels', 'VBZ'), ('words', 'NNS'), ('as', 'IN'), ('noun', 'NN'), (',', ','), ('adjective', 'JJ'), (',', ','), ('verb', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')]\n",
      "[('PoS', 'NNP'), ('tagging', 'VBG'), ('also', 'RB'), ('covers', 'VBZ'), ('tenses', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokenized=nltk.sent_tokenize(sample) #Tokenize the corpora into sentences\n",
    "for i in tokenized:\n",
    "    words=nltk.word_tokenize(i) #Tokenzie each sentence into words\n",
    "    tagged=nltk.pos_tag(words) #tag the words\n",
    "    print (tagged) # output is a list of tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPOS tag list:\\n\\nCC\\tcoordinating conjunction\\nCD\\tcardinal digit\\nDT\\tdeterminer\\nEX\\texistential there (like: \"there is\" ... think of it like \"there exists\")\\nFW\\tforeign word\\nIN\\tpreposition/subordinating conjunction\\nJJ\\tadjective\\t\\'big\\'\\nJJR\\tadjective, comparative\\t\\'bigger\\'\\nJJS\\tadjective, superlative\\t\\'biggest\\'\\nLS\\tlist marker\\t1)\\nMD\\tmodal\\tcould, will\\nNN\\tnoun, singular \\'desk\\'\\nNNS\\tnoun plural\\t\\'desks\\'\\nNNP\\tproper noun, singular\\t\\'Harrison\\'\\nNNPS\\tproper noun, plural\\t\\'Americans\\'\\nPDT\\tpredeterminer\\t\\'all the kids\\'\\nPOS\\tpossessive ending\\tparent\\'s\\nPRP\\tpersonal pronoun\\tI, he, she\\nPRP$\\tpossessive pronoun\\tmy, his, hers\\nRB\\tadverb\\tvery, silently,\\nRBR\\tadverb, comparative\\tbetter\\nRBS\\tadverb, superlative\\tbest\\nRP\\tparticle\\tgive up\\nTO\\tto\\tgo \\'to\\' the store.\\nUH\\tinterjection\\terrrrrrrrm\\nVB\\tverb, base form\\ttake\\nVBD\\tverb, past tense\\ttook\\nVBG\\tverb, gerund/present participle\\ttaking\\nVBN\\tverb, past participle\\ttaken\\nVBP\\tverb, sing. present, non-3d\\ttake\\nVBZ\\tverb, 3rd person sing. present\\ttakes\\nWDT\\twh-determiner\\twhich\\nWP\\twh-pronoun\\twho, what\\nWP$\\tpossessive wh-pronoun\\twhose\\nWRB\\twh-abverb\\twhere, when\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent\\'s\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
