{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. check if adding a feature statistically significant increase the prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal 1: use many other classifier to do text classification\n",
    "# goal 2: show how to text the value of a feature rigourously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "# BernoulliNB works when the features are binary value\n",
    "# MultinomialNB works when the features are discrete\n",
    "#when the feature is not binary or multinimial bydefault GaussianNB is implemented.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "import statistics\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the object set\n",
    "allname=[(name,'male') for name in names.words('male.txt')]+[(name,'female') for name in names.words('female.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the baseline feature function\n",
    "def feature1(text):\n",
    "    return {'last_letter':text[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the testing feature\n",
    "def feature2(text):\n",
    "    return {'last_letter':text[-1], 'first_letter':text[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset1=[(feature1(name),clas) for name,clas in allname]\n",
    "featureset2=[(feature2(name),clas) for name,clas in allname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsize=int(0.8*len(featureset1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classifier objects\n",
    "BNB=SklearnClassifier(BernoulliNB())\n",
    "MNB=SklearnClassifier(MultinomialNB())\n",
    "L1=SklearnClassifier(LogisticRegression(solver='newton-cg'))\n",
    "L2=SklearnClassifier(LogisticRegression(solver='liblinear'))\n",
    "SVC=SklearnClassifier(SVC(gamma='auto'))\n",
    "LSVC=SklearnClassifier(LinearSVC())\n",
    "NuSVC=SklearnClassifier(NuSVC(gamma='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to test accuracy\n",
    "def accuracy(featureset):\n",
    "    accuracy=[]\n",
    "    for i in range(10):\n",
    "#         print(i)\n",
    "        random.shuffle(featureset)\n",
    "        train=featureset[:trainsize]\n",
    "        test=featureset[trainsize:]\n",
    "        nltkNB=nltk.NaiveBayesClassifier.train(train)\n",
    "        nltkDT=nltk.DecisionTreeClassifier.train(train)\n",
    "        BNB.train(train);MNB.train(train);L1.train(train);L2.train(train)\n",
    "        SVC.train(train);LSVC.train(train);NuSVC.train(train)\n",
    "        # we report accuracy using a list of tuple\n",
    "        # each tuple represent each iteration\n",
    "        # [(acc1,acc2,...,acc9),(acc1,acc2,...,acc9),(acc1,acc2,...,acc9)....]\n",
    "        acc1=nltk.classify.accuracy(nltkNB,test)\n",
    "        acc2=nltk.classify.accuracy(nltkDT,test)\n",
    "        acc3=nltk.classify.accuracy(BNB,test)\n",
    "        acc4=nltk.classify.accuracy(MNB,test)\n",
    "        acc5=nltk.classify.accuracy(L1,test)\n",
    "        acc6=nltk.classify.accuracy(L2,test)\n",
    "        acc7=nltk.classify.accuracy(SVC,test)\n",
    "        acc8=nltk.classify.accuracy(LSVC,test)\n",
    "        acc9=nltk.classify.accuracy(NuSVC,test)\n",
    "        accuracy.append((acc1,acc2,acc3,acc4,acc5,acc6,acc7,acc8,acc9))\n",
    "    print([statistics.mean(x) for x in accuracy])\n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7642122928466542, 0.7564505978602895, 0.7575694007412069, 0.7498077057548422, 0.7518355359765051, 0.763373190685966, 0.759876931683099, 0.7601566324033284, 0.7656807216278582, 0.7671491504090623]\n",
      "[0.7778477029578351, 0.766869449688833, 0.7786868051185232, 0.7879868540661492, 0.7656107964478008, 0.7563107475001748, 0.7736521921543948, 0.7766589748968603, 0.7644919935668835, 0.7742115935948535]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "accuracy_baseline=accuracy(featureset1)\n",
    "accuracy_testing=accuracy(featureset2)\n",
    "print(accuracy_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw inference on if the added feature statistically significantly improve the predictive power\n",
    "# featureset 1: [(acc1,acc2,...,acc9),(acc1,acc2,...,acc9),(acc1,acc2,...,acc9)....]\n",
    "# featuerset 2: [(acc1,acc2,...,acc9),(acc1,acc2,...,acc9),(acc1,acc2,...,acc9)....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-3.144519381413715, pvalue=0.005675767514500799)\n",
      "Ttest_indResult(statistic=-4.185171494118548, pvalue=0.000617477877669631)\n",
      "Ttest_indResult(statistic=-1.8610658173765826, pvalue=0.07929739174009337)\n",
      "Ttest_indResult(statistic=-3.1788065244158537, pvalue=0.005262574801584489)\n",
      "Ttest_indResult(statistic=-3.806503546591098, pvalue=0.0013118968975974134)\n",
      "Ttest_indResult(statistic=-3.806503546591098, pvalue=0.0013118968975974134)\n",
      "Ttest_indResult(statistic=-0.027797445995329642, pvalue=0.9781324701546347)\n",
      "Ttest_indResult(statistic=-3.721408969022572, pvalue=0.0015633335250358291)\n",
      "Ttest_indResult(statistic=0.20417215141764028, pvalue=0.8405096865287742)\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    acc_baseline=[x[i] for x in accuracy_baseline]\n",
    "    acc_testing=[x[i] for x in accuracy_testing]\n",
    "    ttest=stats.ttest_ind(acc_baseline,acc_testing,equal_var=False)#paired t-test to calculate if the acc from featureset1 are lower/higher than featureset1 \n",
    "    print(ttest)\n",
    "#A p-value less than 0.05 (typically â‰¤ 0.05) is statistically significant. ... \n",
    "#A p-value higher than 0.05 (> 0.05) is not statistically significant and \n",
    "#indicates strong evidence for the null hypothesis. This means we retain the null hypothesis \n",
    "#and reject the alternative hypothesis.\n",
    "\n",
    "#eg: pvalue=0.9781324701546347 is very large. not statistically significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. variable number of arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addup(*numbers): # define a series of inputs\n",
    "    summation=0\n",
    "    for each in numbers:\n",
    "        summation+=each\n",
    "    return summation\n",
    "# the variable numbers is a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(addup(4,3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2000, 400, 500)\n"
     ]
    }
   ],
   "source": [
    "# another example\n",
    "class employee:\n",
    "    \n",
    "    def __init__(self,*sales):\n",
    "        self.sales=sales\n",
    "    \n",
    "    def printallsale(self):\n",
    "        print(self.sales)\n",
    "        \n",
    "emp1=employee(100,2000,400,500)\n",
    "emp1.printallsale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class employee:\n",
    "    \n",
    "    def __init__(self,*sales):\n",
    "        self.sales=sales\n",
    "    \n",
    "    def printallsale(self):\n",
    "        print(self.sales)\n",
    "        \n",
    "emp1=employee(100,2000,400,500)\n",
    "emp1.printallsale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. build a cooperate classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cooperative classifier\n",
    "# suppose we have 9 classifiers. If 6 votes for male, and 3 votes for female, \n",
    "# then we classify this object to male with a 6/9=66.7% confidence\n",
    "#sometimes the prediction results of the classifiers are different. So create a voting mechansim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "# BernoulliNB works when the features are binary value\n",
    "# MultinomialNB works when the features are discrete\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.corpus import names\n",
    "import statistics\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifier object\n",
    "BNB=SklearnClassifier(BernoulliNB())\n",
    "MNB=SklearnClassifier(MultinomialNB())\n",
    "L1=SklearnClassifier(LogisticRegression(solver='newton-cg'))\n",
    "L2=SklearnClassifier(LogisticRegression(solver='liblinear'))\n",
    "SVC=SklearnClassifier(SVC(gamma='auto'))\n",
    "LSVC=SklearnClassifier(LinearSVC())\n",
    "NuSVC=SklearnClassifier(NuSVC(gamma='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(NuSVC(break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "      max_iter=-1, nu=0.5, probability=False, random_state=None, shrinking=True,\n",
       "      tol=0.001, verbose=False))>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classifiers\n",
    "allname=[(name,'male') for name in names.words('male.txt')]+[(name,'female') for name in names.words('female.txt')]\n",
    "def feature(text):\n",
    "    return {'last_letter':text[-1], 'first_letter':text[0]}\n",
    "featureset=[(feature(name),clas) for name,clas in allname]\n",
    "random.shuffle(featureset)\n",
    "trainsize=int(0.8*len(featureset))\n",
    "train=featureset[:trainsize]\n",
    "test=featureset[trainsize:]#list of tuple. 1st element is he feature and 2nd element is the class.\n",
    "nltkNB=nltk.NaiveBayesClassifier.train(train)\n",
    "nltkDT=nltk.DecisionTreeClassifier.train(train)\n",
    "BNB.train(train);MNB.train(train);L1.train(train);L2.train(train)\n",
    "SVC.train(train);LSVC.train(train);NuSVC.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the cooperative classifier class\n",
    "class Cooperative_classifier:\n",
    "    \n",
    "    def __init__(self,*classifiers):\n",
    "        self.classifier=classifiers\n",
    "        self.vote=None\n",
    "        \n",
    "    def classify(self,feature):#input feature\n",
    "        vote=[]\n",
    "        for i in self.classifier:\n",
    "            res=i.classify(feature)\n",
    "            vote.append(res)\n",
    "        self.vote=vote\n",
    "        return statistics.mode(vote)#mode is the most frequent element in the list. for confidence we take out the most common element.\n",
    "    \n",
    "    def confidence(self):\n",
    "        return self.vote.count(statistics.mode(self.vote))/len(self.vote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=Cooperative_classifier(nltkNB,nltkDT,BNB,MNB,L1,L2,SVC,LSVC,NuSVC)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.788546255506608\n"
     ]
    }
   ],
   "source": [
    "# calcualte the accuracy for the cooperative classifier\n",
    "num_correct=0\n",
    "num_attempt=0\n",
    "for i in test:\n",
    "    num_attempt+=1\n",
    "    feature=i[0]\n",
    "    true_class=i[1]\n",
    "    predicted=C.classify(feature)\n",
    "    if predicted==true_class:\n",
    "        num_correct+=1\n",
    "\n",
    "print(num_correct/num_attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
